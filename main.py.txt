# main.py
import os
import traceback
import logging
from flask import Flask, request, jsonify
from flask_cors import CORS
from supabase import create_client, Client
from ortools.sat.python import cp_model
from datetime import datetime, date, timedelta
from concurrent.futures import ThreadPoolExecutor

# --- Initialization ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logging.info("Python module 'main.py' loaded by Gunicorn worker.")

app = Flask(__name__)
CORS(app) # Enable Cross-Origin Resource Sharing

# These are set as environment variables in Cloud Run
supabase_url = os.environ.get('SUPABASE_URL')
supabase_key = os.environ.get('SUPABASE_KEY')

if not all([supabase_url, supabase_key]):
    raise ValueError("FATAL: Missing required environment variables SUPABASE_URL or SUPABASE_KEY.")

# --- Thread-safe Supabase Client Getter ---
def get_supabase_client() -> Client:
    """Creates a new Supabase client. To be used within each thread."""
    return create_client(supabase_url, supabase_key)

# --- Solver Logic ---
def run_solver_task(run_id: str):
    """The main solver function, designed to be run in a background thread."""
    supabase = get_supabase_client()
    logging.info(f"[{run_id}] Solver task started.")

    try:
        # 1. Update Run Status to SOLVING
        def update_progress(progress: int, status_text: str = 'SOLVING'):
            logging.info(f"[{run_id}] Progress: {progress}%, Status: {status_text}")
            supabase.table('runs').update({'status': status_text, 'progress': progress}).eq('id', run_id).execute()
        
        update_progress(1)

        # 2. Fetch Run and Resource Data
        run_data = supabase.table('runs').select('start_date, end_date').eq('id', run_id).single().execute().data
        if not run_data:
            raise Exception("Could not fetch run data from Supabase.")
        
        start_date = datetime.strptime(run_data['start_date'], '%Y-%m-%d').date()
        end_date = datetime.strptime(run_data['end_date'], '%Y-%m-%d').date()
        
        update_progress(5, 'Fetching resources...')
        resources_data = supabase.table('resources').select('*').eq('isArchived', False).execute().data
        if not resources_data:
            raise Exception("No resources found in Supabase.")

        logging.info(f"[{run_id}] Fetched {len(resources_data)} resources. Scheduling from {start_date} to {end_date}.")
        update_progress(15, 'Building optimization model...')

        # 3. Initialize CP-SAT Model
        model = cp_model.CpModel()
        
        # 4. Define Variables
        # For each resource, create a boolean variable for each possible day it can be scheduled
        task_vars = {}
        all_days = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]

        for r in resources_data:
            for day in all_days:
                var = model.NewBoolVar(f"task_{r['id']}_on_{day.isoformat()}")
                task_vars[(r['id'], day)] = var

        update_progress(30, 'Creating variables and constraints...')
        
        # 5. Add Constraints
        # Constraint 1: Each task must be scheduled exactly once
        for r in resources_data:
            model.AddExactlyOne([task_vars[(r['id'], day)] for day in all_days])

        # Constraint 2: Daily total study time cannot exceed a budget (e.g., 8 hours = 480 mins)
        daily_time_budget = 480 # 8 hours
        for day in all_days:
            daily_duration = sum(
                r['durationMinutes'] * task_vars[(r['id'], day)] for r in resources_data
            )
            model.Add(daily_duration <= daily_time_budget)
            
        # Add more constraints here as needed (e.g., resource ordering, frequency)
        
        update_progress(60, 'Solving schedule... this is the longest step.')
        
        # 6. Solve the model
        solver = cp_model.CpSolver()
        solver.parameters.max_time_in_seconds = 300.0 # 5 minute timeout
        status = solver.Solve(model)

        # 7. Process and Save Results
        if status == cp_model.OPTIMAL or status == cp_model.FEASIBLE:
            logging.info(f"[{run_id}] Solver found a solution. Processing results...")
            update_progress(90, 'Finalizing and saving results...')
            
            schedule_slots = []
            for r in resources_data:
                for day in all_days:
                    if solver.Value(task_vars[(r['id'], day)]) == 1:
                        # Simple sequential placement within the day for this example
                        # A real implementation would solve for start/end times too
                        slot = {
                            "run_id": run_id,
                            "resource_id": r['id'],
                            "date": day.isoformat(),
                            "start_minute": 0, # Placeholder
                            "end_minute": r['durationMinutes'], # Placeholder
                            "title": r['title'],
                            "domain": r['domain'],
                            "type": r['type'],
                        }
                        schedule_slots.append(slot)
            
            # Batch insert results
            if schedule_slots:
                supabase.table('schedule_slots').insert(schedule_slots).execute()
                
            update_progress(100, 'COMPLETE')
            logging.info(f"[{run_id}] Successfully saved {len(schedule_slots)} schedule slots.")

        else:
            error_message = f"Solver could not find a solution. Status: {solver.StatusName(status)}"
            logging.error(f"[{run_id}] {error_message}")
            supabase.table('runs').update({'status': 'FAILED', 'error_text': error_message}).eq('id', run_id).execute()

    except Exception as e:
        error_message = f"An unexpected error occurred in the solver task: {str(e)}"
        logging.error(f"[{run_id}] {error_message}\n{traceback.format_exc()}")
        # Use a fresh client in case the old one is stale
        get_supabase_client().table('runs').update({'status': 'FAILED', 'error_text': error_message}).eq('id', run_id).execute()

# --- Flask Endpoints ---
@app.route('/solve', methods=['POST'])
def solve_endpoint():
    """Receives a request to start a solver job."""
    logging.info("Request received at /solve endpoint.")
    data = request.json
    run_id = data.get('run_id')

    if not run_id:
        logging.error("Received request without a run_id.")
        return jsonify({'error': 'run_id is required'}), 400

    logging.info(f"[{run_id}] Handing off to background solver task.")
    # Run the time-consuming solver task in a background thread
    executor = ThreadPoolExecutor(max_workers=1)
    executor.submit(run_solver_task, run_id)
    
    # Immediately return a 202 Accepted response to the caller
    return jsonify({'status': 'Solver job accepted', 'run_id': run_id}), 202

@app.route('/', methods=['GET'])
def health_check():
    """A simple health check endpoint."""
    return "Radiology Solver is running.", 200

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=int(os.environ.get('PORT', 8080)))
