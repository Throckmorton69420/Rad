import os
import traceback
from flask import Flask, request, jsonify
from supabase import create_client, Client
from ortools.sat.python import cp_model
from datetime import date, timedelta, datetime

# --- Initialization ---
app = Flask(__name__)

# These are set as environment variables in Cloud Run
supabase_url = os.environ.get("SUPABASE_URL")
supabase_key = os.environ.get("SUPABASE_KEY")
solver_token = os.environ.get("INTERNAL_API_TOKEN") # This is now optional but good for defense-in-depth

if not all([supabase_url, supabase_key]):
    # Note: solver_token is not strictly required for this architecture to work,
    # as OIDC provides the primary authentication.
    raise ValueError("FATAL: Missing required environment variables SUPABASE_URL or SUPABASE_KEY.")

supabase: Client = create_client(supabase_url, supabase_key)

# --- Synchronous Solver Logic ---
def solve_schedule(run_id: str):
    """
    This function contains the long-running solver logic.
    It runs synchronously and is triggered by a POST request from Google Cloud Tasks.
    Robust error handling is included to ensure the run status is always updated.
    """
    try:
        app.logger.info(f"[{run_id}] Solver process started.")
        supabase.table("runs").update({"status": "SOLVING"}).eq("id", run_id).execute()

        # 1. Fetch all necessary data from Supabase
        run_data_req = supabase.table("runs").select("start_date, end_date").eq("id", run_id).single().execute()
        if not run_data_req.data:
            raise Exception(f"Run ID {run_id} not found.")
        run_data = run_data_req.data
        app.logger.info(f"[{run_id}] Fetched run data.")

        all_resources_req = supabase.table("resources").select("*").eq("isArchived", False).execute()
        all_resources = all_resources_req.data
        app.logger.info(f"[{run_id}] Fetched {len(all_resources)} resources.")

        user_data_req = supabase.table("user_data").select("data").eq("id", 1).single().execute()
        exception_dates = []
        if user_data_req.data and 'data' in user_data_req.data and 'exceptionDates' in user_data_req.data['data']:
            exception_dates = user_data_req.data['data']['exceptionDates']
        app.logger.info(f"[{run_id}] Fetched {len(exception_dates)} exception dates.")

        # --- CP-SAT MODEL CONSTRUCTION ---
        model = cp_model.CpModel()
        
        start_date = datetime.fromisoformat(run_data['start_date']).date()
        end_date = datetime.fromisoformat(run_data['end_date']).date()
        num_days = (end_date - start_date).days + 1
        all_dates = [start_date + timedelta(days=i) for i in range(num_days)]

        tasks = {}
        resource_map = {res['id']: res for res in all_resources}

        app.logger.info(f"[{run_id}] Building model variables...")
        for res in all_resources:
            task_days = [model.NewBoolVar(f"on_day_{res['id']}_{d}") for d in range(num_days)]
            tasks[res['id']] = { 'days': task_days, 'duration': res['durationMinutes'], 'resource': res }
            model.AddAtMostOne(task_days)

        # 2. Add Constraints
        app.logger.info(f"[{run_id}] Adding constraints...")
        exception_map = {e['date']: e for e in exception_dates}
        for d, current_date in enumerate(all_dates):
            date_str = str(current_date)
            exception = exception_map.get(date_str)
            
            daily_budget = 900 # Default 15 hours
            if exception:
                if exception.get('isRestDayOverride', False):
                    daily_budget = 0
                elif exception.get('targetMinutes') is not None:
                    daily_budget = exception['targetMinutes']

            daily_duration = sum(tasks[res_id]['duration'] * tasks[res_id]['days'][d] for res_id in tasks)
            model.Add(daily_duration <= daily_budget)
            if daily_budget == 0:
                model.Add(daily_duration == 0)

        # 3. Add Objectives
        app.logger.info(f"[{run_id}] Adding objectives...")
        priority_map = {'high': 1000, 'medium': 100, 'low': 10}
        model.Maximize(sum(
            priority_map.get(task['resource'].get('schedulingPriority', 'medium'), 100) * sum(task['days'])
            for task in tasks.values()
        ))

        # --- SOLVER EXECUTION ---
        app.logger.info(f"[{run_id}] Starting solver...")
        solver = cp_model.CpSatSolver()
        solver.parameters.max_time_in_seconds = 840 # 14 minutes, safely under the 15 min Cloud Run timeout
        solver.parameters.num_search_workers = 8 # Use multiple cores
        status = solver.Solve(model)
        
        # --- PROCESS AND SAVE RESULTS ---
        app.logger.info(f"[{run_id}] Solver finished with status: {solver.StatusName(status)}")
        if status == cp_model.OPTIMAL or status == cp_model.FEASIBLE:
            schedule_slots = []
            
            for d, current_date in enumerate(all_dates):
                tasks_for_day_ids = [res_id for res_id, task_vars in tasks.items() if solver.Value(task_vars['days'][d])]
                if not tasks_for_day_ids: continue

                tasks_for_day = [resource_map[res_id] for res_id in tasks_for_day_ids]
                
                # Sorting logic to order tasks within a day
                def get_sort_key(resource):
                    source = (resource.get('videoSource') or resource.get('bookSource') or 'zz_custom').lower()
                    res_type = resource.get('type', '').lower()
                    if 'titan' in source: return 0
                    if 'core radiology' in source: return 1
                    if 'crack the core' in source or 'case companion' in source or 'war machine' in source or 'review of physics' in source: return 2
                    if 'guide' in res_type: return 3
                    if 'qbank' in res_type or 'question' in res_type: return 4
                    return 5
                tasks_for_day.sort(key=get_sort_key)

                current_minute = 0
                for resource in tasks_for_day:
                    start_minute = current_minute
                    end_minute = current_minute + resource['durationMinutes']
                    schedule_slots.append({
                        "run_id": run_id, "resource_id": resource['id'], "date": str(current_date),
                        "start_minute": start_minute, "end_minute": end_minute,
                        "title": resource['title'], "domain": resource['domain'], "type": resource['type']
                    })
                    current_minute = end_minute
            
            app.logger.info(f"[{run_id}] Generated {len(schedule_slots)} slots. Inserting into database...")
            if schedule_slots:
                supabase.table("schedule_slots").delete().eq('run_id', run_id).execute()
                supabase.table("schedule_slots").insert(schedule_slots).execute()
            
            supabase.table("runs").update({"status": "COMPLETE"}).eq("id", run_id).execute()
            app.logger.info(f"[{run_id}] Run marked as COMPLETE.")
        else:
            raise Exception(f"Solver could not find a solution. Status: {solver.StatusName(status)}")

    except Exception as e:
        error_trace = traceback.format_exc()
        app.logger.error(f"CRITICAL ERROR during solve for run_id {run_id}: {e}\n{error_trace}")
        supabase.table("runs").update({"status": "FAILED", "error_text": f"Error: {str(e)}\nTrace: {error_trace}"}).eq("id", run_id).execute()


# --- API Endpoints ---
@app.route("/internal/run-solver", methods=["POST"])
def run_solver_endpoint():
    # Primary authentication is handled by Cloud Run's IAM integration (Ingress: Internal).
    # The OIDC token from Cloud Tasks is validated by Google's infrastructure before this code is even hit.
    
    data = request.get_json()
    run_id = data.get("run_id")
    if not run_id:
        app.logger.error("Received task with no run_id.")
        return jsonify({"error": "run_id is required"}), 400
        
    app.logger.info(f"Received internal solver task for run_id: {run_id}")

    # Run the solver synchronously.
    solve_schedule(run_id)
    
    # Return a success response to Cloud Tasks to acknowledge completion.
    return jsonify({"status": "Solver process finished for run_id", "run_id": run_id}), 200

@app.route("/", methods=["GET"])
def health_check():
    # A simple health check endpoint for Cloud Run to verify the container is responsive.
    return "Radiology Solver Service is running."

if __name__ == "__main__":
    app.run(debug=False, host="0.0.0.0", port=int(os.environ.get("PORT", 8080)))
