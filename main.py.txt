import os
import traceback
from flask import Flask, request, jsonify
from flask_cors import CORS
from supabase import create_client, Client
from ortools.sat.python import cp_model
from datetime import date, timedelta, datetime
import logging
from concurrent.futures import ThreadPoolExecutor

# --- Initialization ---
app = Flask(__name__)
CORS(app)  # Enable Cross-Origin Resource Sharing
app.logger.setLevel(logging.INFO)

# These are set as environment variables in Cloud Run
supabase_url = os.environ.get("SUPABASE_URL")
supabase_key = os.environ.get("SUPABASE_KEY")

if not all([supabase_url, supabase_key]):
    raise ValueError("FATAL: Missing required environment variables SUPABASE_URL or SUPABASE_KEY.")

# --- Thread-safe Supabase Client Getter ---
def get_supabase_client() -> Client:
    """Creates a new Supabase client. To be used within each thread."""
    return create_client(supabase_url, supabase_key)

# --- Thread Pool for Background Tasks ---
# Using a thread pool to manage background solver tasks.
executor = ThreadPoolExecutor(max_workers=4) # Allow up to 4 concurrent solves


# --- Background Solver Logic ---
def solve_schedule(run_id: str):
    """
    This function contains the long-running solver logic.
    It's designed to be run in a background thread from a ThreadPoolExecutor.
    It creates its own app context and Supabase client to ensure thread safety.
    """
    with app.app_context():
        app.logger.info(f"[{run_id}] Background thread execution beginning.")
        supabase = get_supabase_client() # Get a fresh, thread-local client
        
        try:
            # IMMEDIATE PROGRESS UPDATE
            app.logger.info(f"[{run_id}] Attempting to update run status to SOLVING and progress to 1%.")
            supabase.table("runs").update({"status": "SOLVING", "progress": 1}).eq("id", run_id).execute()
            app.logger.info(f"[{run_id}] Successfully updated run status. Fetching data...")

            # 1. Fetch all necessary data from Supabase
            run_data_req = supabase.table("runs").select("start_date, end_date").eq("id", run_id).single().execute()
            if not run_data_req.data:
                raise Exception(f"Run ID {run_id} not found.")
            run_data = run_data_req.data
            app.logger.info(f"[{run_id}] Fetched run data for period: {run_data['start_date']} to {run_data['end_date']}.")
            
            supabase.table("runs").update({"progress": 5}).eq("id", run_id).execute()
            app.logger.info(f"[{run_id}] Progress at 5%. Fetching resources...")

            all_resources_req = supabase.table("resources").select("*").eq("isArchived", False).execute()
            all_resources = all_resources_req.data
            app.logger.info(f"[{run_id}] Fetched {len(all_resources)} active resources.")

            exception_dates = []
            try:
                user_data_req = supabase.table("user_data").select("data").eq("id", 1).single().execute()
                if user_data_req.data and 'data' in user_data_req.data and isinstance(user_data_req.data['data'], dict) and 'exceptionDates' in user_data_req.data['data']:
                    exception_dates = user_data_req.data['data']['exceptionDates'] or []
                app.logger.info(f"[{run_id}] Fetched {len(exception_dates)} exception dates.")
            except Exception as user_data_error:
                app.logger.warning(f"[{run_id}] Could not fetch or parse user_data for exceptions, continuing without them. Error: {user_data_error}")
                exception_dates = []
            
            supabase.table("runs").update({"progress": 15}).eq("id", run_id).execute()
            app.logger.info(f"[{run_id}] Progress at 15%. Building CP-SAT model...")

            # --- CP-SAT MODEL CONSTRUCTION ---
            model = cp_model.CpModel()
            
            start_date = datetime.fromisoformat(run_data['start_date']).date()
            end_date = datetime.fromisoformat(run_data['end_date']).date()
            num_days = (end_date - start_date).days + 1
            all_dates = [start_date + timedelta(days=i) for i in range(num_days)]

            tasks = {}
            resource_map = {res['id']: res for res in all_resources}

            app.logger.info(f"[{run_id}] Building model variables...")
            for res in all_resources:
                task_days = [model.NewBoolVar(f"on_day_{res['id']}_{d}") for d in range(num_days)]
                tasks[res['id']] = { 'days': task_days, 'duration': res['durationMinutes'], 'resource': res }
                model.AddAtMostOne(task_days)

            # 2. Add Constraints
            app.logger.info(f"[{run_id}] Adding constraints...")
            exception_map = {e['date']: e for e in exception_dates if isinstance(e, dict) and 'date' in e}
            for d, current_date in enumerate(all_dates):
                date_str = str(current_date)
                exception = exception_map.get(date_str)
                
                daily_budget = 900 # Default 15 hours
                if exception:
                    if exception.get('isRestDayOverride', False):
                        daily_budget = 0
                    elif exception.get('targetMinutes') is not None:
                        daily_budget = exception['targetMinutes']

                daily_duration = sum(tasks[res_id]['duration'] * tasks[res_id]['days'][d] for res_id in tasks)
                model.Add(daily_duration <= daily_budget)
                if daily_budget == 0:
                    model.Add(daily_duration == 0)

            # 3. Add Objectives
            app.logger.info(f"[{run_id}] Adding objectives...")
            priority_map = {'high': 1000, 'medium': 100, 'low': 10}
            model.Maximize(sum(
                priority_map.get(task['resource'].get('schedulingPriority', 'medium'), 100) * sum(task['days'])
                for task in tasks.values()
            ))
            
            supabase.table("runs").update({"progress": 25}).eq("id", run_id).execute()
            app.logger.info(f"[{run_id}] Progress at 25%. Starting solver...")

            # --- SOLVER EXECUTION ---
            solver = cp_model.CpSatSolver()
            solver.parameters.max_time_in_seconds = 840 # 14 minutes
            solver.parameters.num_search_workers = 8 # Use multiple cores
            status = solver.Solve(model)
            
            supabase.table("runs").update({"progress": 85}).eq("id", run_id).execute()
            app.logger.info(f"[{run_id}] Progress at 85%. Solver finished with status: {solver.StatusName(status)}. Processing results...")
            
            # --- PROCESS AND SAVE RESULTS ---
            if status == cp_model.OPTIMAL or status == cp_model.FEASIBLE:
                schedule_slots = []
                
                for d, current_date in enumerate(all_dates):
                    tasks_for_day_ids = [res_id for res_id, task_vars in tasks.items() if solver.Value(task_vars['days'][d])]
                    if not tasks_for_day_ids: continue

                    tasks_for_day = [resource_map[res_id] for res_id in tasks_for_day_ids]
                    
                    def get_sort_key(resource):
                        source = (resource.get('videoSource') or resource.get('bookSource') or 'zz_custom').lower()
                        res_type = resource.get('type', '').lower()
                        if 'titan' in source: return 0
                        if 'core radiology' in source: return 1
                        if 'crack the core' in source or 'case companion' in source or 'war machine' in source or 'review of physics' in source: return 2
                        if 'guide' in res_type: return 3
                        if 'qbank' in res_type or 'question' in res_type: return 4
                        return 5
                    tasks_for_day.sort(key=get_sort_key)

                    current_minute = 0
                    for resource in tasks_for_day:
                        start_minute = current_minute
                        end_minute = current_minute + resource['durationMinutes']
                        schedule_slots.append({
                            "run_id": run_id, "resource_id": resource['id'], "date": str(current_date),
                            "start_minute": start_minute, "end_minute": end_minute,
                            "title": resource['title'], "domain": resource['domain'], "type": resource['type']
                        })
                        current_minute = end_minute
                
                app.logger.info(f"[{run_id}] Generated {len(schedule_slots)} slots. Deleting old slots and inserting new ones...")
                if schedule_slots:
                    supabase.table("schedule_slots").delete().eq('run_id', run_id).execute()
                    insert_res = supabase.table("schedule_slots").insert(schedule_slots).execute()
                    if insert_res.data is None:
                       app.logger.warning(f"[{run_id}] Supabase insert operation might have failed or returned no data.")
                else:
                    app.logger.warning(f"[{run_id}] No schedule slots were generated by the solver.")

                app.logger.info(f"[{run_id}] Database insertion complete. Updating run to COMPLETE.")
                supabase.table("runs").update({"progress": 95}).eq("id", run_id).execute()
                supabase.table("runs").update({"status": "COMPLETE", "progress": 100}).eq("id", run_id).execute()
                app.logger.info(f"[{run_id}] Run marked as COMPLETE.")
            else:
                raise Exception(f"Solver could not find a solution. Status: {solver.StatusName(status)}")

        except Exception as e:
            error_trace = traceback.format_exc()
            app.logger.error(f"CRITICAL ERROR during solve for run_id {run_id}: {e}\n{error_trace}")
            try:
                # Use the same thread-local client to report the failure
                supabase.table("runs").update({"status": "FAILED", "progress": -1, "error_text": f"Error: {str(e)}\nTrace: {error_trace}"}).eq("id", run_id).execute()
                app.logger.info(f"[{run_id}] Successfully marked run as FAILED in database.")
            except Exception as db_update_error:
                app.logger.error(f"[{run_id}] FAILED TO MARK RUN AS FAILED. DB update error: {db_update_error}")


# --- API Endpoints ---
@app.route("/solve", methods=["POST"])
def run_solver_endpoint():
    # This log is CRITICAL.
    app.logger.info("Request received at /solve endpoint.")
    
    auth_header = request.headers.get("Authorization")
    if not auth_header:
        app.logger.warning("Request to /solve is missing Authorization header. This will be rejected by Cloud Run's IAM if not invoked by an authenticated principal.")

    data = request.get_json()
    if not data or 'run_id' not in data:
        app.logger.error("Received solver request with no run_id in body.")
        return jsonify({"error": "run_id is required in the JSON body"}), 400
    
    run_id = data.get("run_id")
    app.logger.info(f"Request validated for run_id: {run_id}. Submitting task to ThreadPoolExecutor.")
    
    executor.submit(solve_schedule, run_id)
    
    app.logger.info(f"Task for {run_id} submitted. Returning 202 Accepted.")
    return jsonify({"status": "Solver process started in background.", "run_id": run_id}), 202

@app.route("/", methods=["GET"])
def health_check():
    return "Radiology Solver Service is running."

if __name__ == "__main__":
    app.run(debug=False, host="0.0.0.0", port=int(os.environ.get("PORT", 8080)))